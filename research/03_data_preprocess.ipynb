{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\senthil\\\\project\\\\End_to_end_text_classification_using_bert'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    indepent_feature: list\n",
    "    drop_feature: list\n",
    "    target_feature: str\n",
    "    preprocess_data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassification.constants import *\n",
    "from textClassification.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_preprocess_config(self) -> DataPreprocessConfig:\n",
    "        config = self.config.data_preprocess\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocess_config = DataPreprocessConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            data_path =config.data_path,\n",
    "            indepent_feature= config.indepent_feature,\n",
    "            drop_feature= config.drop_feature,\n",
    "            target_feature= config.target_feature,\n",
    "            preprocess_data_path= config.preprocess_data_path,\n",
    "        )\n",
    "\n",
    "        return data_preprocess_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textClassification.logging import logging\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "stemmer = PorterStemmer()\n",
    "lemat = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, config: DataPreprocessConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def feature_text_process(self,data_frame,feature):\n",
    "        #remove stopwords and apply lemmatization for all data\n",
    "        corpus =[]\n",
    "        tags_re = re.compile('<.*?>')\n",
    "        for i in range(len(data_frame[feature])):\n",
    "            #text = BeautifulSoup(df['Full_Article'][i],'lxml').text\n",
    "            text = re.sub(tags_re,'',data_frame[feature][i])\n",
    "            text = re.sub('[^a-zA-Z]',' ',data_frame[feature][i])\n",
    "            text = text.lower()\n",
    "            text = text.split()\n",
    "            review = [lemat.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "            review = ' '.join(review)\n",
    "            review = review[1:len(review)-1]\n",
    "            corpus.append(review)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "\n",
    "    def convert(self):\n",
    "        data_path = self.config.data_path\n",
    "        df = pd.read_csv(data_path,encoding='cp1252')\n",
    "        drop_feature = self.config.drop_feature\n",
    "\n",
    "        #drop feature\n",
    "        df.drop(drop_feature,axis=1,inplace=True)\n",
    "\n",
    "        #data Preprocssing and store preprocessed data into new column\n",
    "\n",
    "        indepent_feature = self.config.indepent_feature\n",
    "        print('#######################',df.info())\n",
    "        preprocess_column = ['Heading_preprocess','Description_preprocess','Article_preprocess']\n",
    "        for i in range(len(indepent_feature)):\n",
    "            preprocess_feature= self.feature_text_process(df,indepent_feature[i])\n",
    "            df[preprocess_column[i]] = preprocess_feature\n",
    "        \n",
    "        #convert cat_feature in numeric representation using labelencoder\n",
    "        target_feature = self.config.target_feature\n",
    "        df[target_feature]= le.fit_transform(df['Article_Type'])\n",
    "\n",
    "        #train test split \n",
    "\n",
    "        X = df[preprocess_column]\n",
    "        y = df['Article_Type']\n",
    "\n",
    "        X_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "        train = pd.concat([X_train,y_train],axis =1)\n",
    "        test = pd.concat([X_test,y_test],axis =1)\n",
    "\n",
    "        preprocess_path = os.path.join(self.config.preprocess_data_path)\n",
    "        \n",
    "        \n",
    "        train.to_csv(preprocess_path+'train.csv')\n",
    "        test.to_csv(preprocess_path+'test.csv')\n",
    "\n",
    "        print('#######################',train.info())\n",
    "\n",
    "        print('!!!!!!!!!!!!!!!!!!!!!',test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4305 entries, 0 to 4304\n",
      "Data columns (total 4 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Heading              4305 non-null   object\n",
      " 1   Article.Description  4305 non-null   object\n",
      " 2   Full_Article         4305 non-null   object\n",
      " 3   Article_Type         4305 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 134.7+ KB\n",
      "####################### None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3444 entries, 1302 to 860\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Heading_preprocess      3444 non-null   object\n",
      " 1   Description_preprocess  3444 non-null   object\n",
      " 2   Article_preprocess      3444 non-null   object\n",
      " 3   Article_Type            3444 non-null   int32 \n",
      "dtypes: int32(1), object(3)\n",
      "memory usage: 121.1+ KB\n",
      "####################### None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 861 entries, 274 to 353\n",
      "Data columns (total 4 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   Heading_preprocess      861 non-null    object\n",
      " 1   Description_preprocess  861 non-null    object\n",
      " 2   Article_preprocess      861 non-null    object\n",
      " 3   Article_Type            861 non-null    int32 \n",
      "dtypes: int32(1), object(3)\n",
      "memory usage: 30.3+ KB\n",
      "!!!!!!!!!!!!!!!!!!!!! None\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_preprocess_config = config.get_data_preprocess_config()\n",
    "    data_preprocess = DataPreprocessor(config=data_preprocess_config)\n",
    "    data_preprocess.convert()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

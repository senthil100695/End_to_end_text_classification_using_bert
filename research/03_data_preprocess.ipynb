{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\senthil\\\\project\\\\End_to_end_text_classification_using_bert'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataPreprocessConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    indepent_feature: list\n",
    "    drop_feature: list\n",
    "    target_feature: str\n",
    "    preprocess_data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textClassification.constants import *\n",
    "from textClassification.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_preprocess_config(self) -> DataPreprocessConfig:\n",
    "        config = self.config.data_preprocess\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_preprocess_config = DataPreprocessConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            data_path =config.data_path,\n",
    "            indepent_feature= config.indepent_feature,\n",
    "            drop_feature= config.drop_feature,\n",
    "            target_feature= config.target_feature,\n",
    "            preprocess_data_path= config.preprocess_data_path,\n",
    "        )\n",
    "\n",
    "        return data_preprocess_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from textClassification.logging import logging\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "le = LabelEncoder()\n",
    "stemmer = PorterStemmer()\n",
    "lemat = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, config: DataPreprocessConfig):\n",
    "        self.config = config\n",
    "\n",
    "\n",
    "    \n",
    "    def feature_text_process(self,data_frame,feature):\n",
    "        #remove stopwords and apply lemmatization for all data\n",
    "        corpus =[]\n",
    "        tags_re = re.compile('<.*?>')\n",
    "        for i in range(len(data_frame[feature])):\n",
    "            #text = BeautifulSoup(df['Full_Article'][i],'lxml').text\n",
    "            text = re.sub(tags_re,'',data_frame[feature][i])\n",
    "            text = re.sub('[^a-zA-Z]',' ',data_frame[feature][i])\n",
    "            text = text.lower()\n",
    "            text = text.split()\n",
    "            review = [lemat.lemmatize(word) for word in text if not word in set(stopwords.words('english'))]\n",
    "            review = ' '.join(review)\n",
    "            review = review[1:len(review)-1]\n",
    "            corpus.append(review)\n",
    "        return corpus\n",
    "        \n",
    "    \n",
    "\n",
    "    def convert(self):\n",
    "        data_path = self.config.data_path\n",
    "        df = pd.read_csv(data_path,encoding='cp1252')\n",
    "        drop_feature = self.config.drop_feature\n",
    "\n",
    "        #drop feature\n",
    "        df.drop(drop_feature,axis=1,inplace=True)\n",
    "\n",
    "        #data Preprocssing and store preprocessed data into new column\n",
    "\n",
    "        indepent_feature = self.config.indepent_feature\n",
    "        preprocess_column = ['Heading_preprocess','Description_preprocess','Article_preprocess']\n",
    "        for i in range(len(indepent_feature)):\n",
    "            preprocess_feature= self.feature_text_process(df,indepent_feature[i])\n",
    "            df[preprocess_column[i]] = preprocess_feature\n",
    "        \n",
    "        #convert cat_feature in numeric representation using labelencoder\n",
    "        target_feature = self.config.target_feature\n",
    "        df[target_feature]= le.fit_transform(df['Article_Type'])\n",
    "\n",
    "        #train test split \n",
    "\n",
    "        X = df[preprocess_column]\n",
    "        y = df['Article_Type']\n",
    "\n",
    "        X_train, X_test , y_train, y_test = train_test_split(X,y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "        train = pd.concat([X_train,y_train],axis =1)\n",
    "        test = pd.concat([X_test,y_test],axis =1)\n",
    "\n",
    "        preprocess_path = os.path.join(self.config.preprocess_data_path)\n",
    "        \n",
    "\n",
    "        train.to_csv(preprocess_path+'train.csv')\n",
    "        test.to_csv(preprocess_path+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_preprocess_config = config.get_data_preprocess_config()\n",
    "    data_preprocess = DataPreprocessor(config=data_preprocess_config)\n",
    "    data_preprocess.convert()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
